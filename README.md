# Part-Whole Relational Fusion Towards Multi-Modal Scene Understanding



![](./figs/Introduction.png)

## Introduction
Multi-modal fusion has played a vital role in multi-modal scene understanding. Most existing methods focus on cross-modal fusion involving two modalities, often overlooking more complex multi-modal fusion, which is essential for real-world applications. Thererfore we propose a Part-Whole Relational Fusion (PWRF) framework. For the first time, this framework treats multi-modal fusion as part-whole relational fusion. It routes multiple individual part-level modalities to a fused whole level modality using the part-whole relational routing ability of Capsule Networks (CapsNets). Through this part-whole routing, our PWRF generates modal-shared and modal-specific semantics from the whole-level modal capsules and the routing coefficients, respectively. 

## Task validation
On top of that, modal-shared and modal-specific details can be employed to solve the issue of multi-modal scene understanding, including synthetic arbitrary-modal semantic segmentation and visible-depth-thermal salient object detection. Experiments on several datasets demonstrate the superiority of the proposed PWRF framework for multi-modal scene understanding. 

## Multi-modal semantic segmentation
The details will be updated soon.

## Visible-depth-thermal salient object detection
The details will be updated soon.




